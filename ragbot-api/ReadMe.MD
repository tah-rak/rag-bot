---

# RAG-PDF-API

## ğŸ“Œ Overview  
The **RAG-PDF-API** enables users to upload PDF documents and ask questions, receiving intelligent responses based on the document content. It utilizes **Retrieval-Augmented Generation (RAG)** with **LLMs, embeddings, and a vector database** for efficient document-based querying.

### ğŸ”¹ Key Technologies:
- **LLM & Embeddings**: OpenAI APIs (GPT-4 & `text-embedding-ada-002`)
- **Vector Storage**: Pinecone
- **Backend**: FastAPI (Python)
- **Frontend**: React
- **PDF Processing**: PyPDF2 (No OCR support)

---

## ğŸ—ï¸ System Architecture

### **High-Level Architecture Flow**

1. **User Interaction**

   - Users upload PDFs and submit queries via the **React frontend**.
   - The system processes queries using OpenAI's **LLM and stored vector embeddings**.

2. **Backend (FastAPI)**

   - Manages API endpoints for PDF upload, query retrieval, and LLM interaction.
   - Routes requests to different services (**document processing, vector storage, and query handling**).

3. **PDF Processing Module**

   - Extracts text from PDFs using **PyPDF2**.
   - Chunks text into **meaningful sections** for efficient retrieval.
   - Cleans and preprocesses extracted text.

4. **Embedding Model (OpenAI)**

   - Converts text chunks into vector embeddings using `text-embedding-ada-002`.
   - Stores embeddings in **Pinecone** for quick retrieval.

5. **Vector Database (Pinecone)**

   - Stores document chunks and corresponding **vector embeddings**.
   - Enables **fast similarity search** for retrieving relevant text sections.

6. **Retrieval & Context Builder**

   - Converts user queries into **vector embeddings**.
   - Searches Pinecone for **relevant document chunks**.
   - Constructs context from retrieved text for **LLM response generation**.

7. **LLM**

   - Processes user queries along with the **retrieved document context**.
   - Generates a well-structured response based on the available context.

8. **Response to User**
   - The chatbot sends the **final response** back to the frontend.
   - The response is displayed in the chat interface.

---

## ğŸ”„ Data Flow Process

1ï¸âƒ£ **User Uploads a PDF** â†’ The file is sent to the backend for processing.  
2ï¸âƒ£ **Text Extraction & Chunking** â†’ Extracted text is cleaned and divided into smaller **chunks**.  
3ï¸âƒ£ **Embedding Generation** â†’ Each chunk is converted into a **vector embedding** using OpenAI.  
4ï¸âƒ£ **Storing in Pinecone** â†’ Vector embeddings are stored for **future retrieval**.  
5ï¸âƒ£ **User Submits Query** â†’ The system retrieves **relevant document chunks** from Pinecone.  
6ï¸âƒ£ **Context Building** â†’ The best-matching chunks are formatted as input to GPT-4.  
7ï¸âƒ£ **Response Generation** â†’ GPT generates an answer based on the **retrieved context**.  
8ï¸âƒ£ **Response Display** â†’ The response is sent back to the frontend and shown to the user.

---

## ğŸ› ï¸ Evaluation of Responses

After querying the **RAG-PDF-API**, response evaluation is crucial. **LlamaIndex** provides several evaluation methods to ensure response quality.

### **Relevance Evaluation (No Ground Truth Needed)**

- Use **LlamaIndex's `RelevancyEvaluator`** to assess whether the generated responses are **relevant** based on the retrieved document context.
- No labeled datasets are required for this type of evaluation.

### ğŸ”— **Useful Links**

- [LlamaIndex Relevance Evaluation Guide](https://docs.llamaindex.ai/en/module_guides/evaluating/usage_pattern.html)
- [LlamaIndex Relevancy Evaluator](https://docs.llamaindex.ai/en/stable/examples/evaluation/relevancy_eval.html)
- [Evaluating Search with Human Judgment](https://dtunkelang.medium.com/evaluating-search-using-human-judgement-fbb2eeba37d9)

---

## ğŸ“Œ Pending Improvements

- [ ] **Implement Namespace Separation**
- [ ] **Implement Cloud-Based File Storage**
- [ ] **Relevancy Check (Optional)**

---

## ğŸš€ Getting Started

### **1ï¸âƒ£ Clone the Repository**

```sh
git clone https://github.com/your-repo/rag-pdf-api.git
cd rag-pdf-api
```

### **2ï¸âƒ£ Install Dependencies**

```sh
pip install -r requirements.txt
```

### **3ï¸âƒ£ Run the Backend**

```sh
uvicorn main:app --reload
```

### **4ï¸âƒ£ Start the Frontend**

```sh
cd frontend
npm install
npm start
```

---

## ğŸ¯ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

---

## ğŸ“œ License

MIT License. See `LICENSE` for details.

---
